\documentclass[12pt, a4paper]{article}
\usepackage[margin = 2cm]{geometry}
% Vertical text spacing
\parindent = 0cm \parskip = 0cm
% Section
\usepackage[compact]{titlesec} \titlespacing*{\section}{0pt}{2ex}{2ex}
\titleformat*{\section}{\normalfont\Large\bfseries\color[RGB]{0,0,192}}
\titleformat*{\subsection}{\normalfont\bfseries\color[RGB]{192,0,0}}
% Table spacing
\newcommand\TS{\rule{0pt}{2.6ex}}         % Top strut
\newcommand\BS{\rule[-0.9ex]{0pt}{0pt}}   % Bottom strut
\usepackage{array, multirow}
\newcolumntype{L}[1]{>{\raggedright\TS\BS\arraybackslash}m{#1}}
\newcolumntype{C}[1]{>{\centering  \TS\BS\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft \TS\BS\arraybackslash}m{#1}}
% Equations
\usepackage{amsmath, bm, bbold, tikz}

\title{\vspace{-6ex} CO395 Group 57 \vspace{-1ex}}
\author{Dongxiao Huang, Zheng Xun Phang, Yufeng Zhang}
\date{\vspace{-3ex}}

\begin{document}
\maketitle
\newcommand\ones{\bm{1}}

\section*{Q1}

\subsection*{Linear layers}
If each image in $X$ is reshaped into a row vector, then the forward pass can be written as
\[ z = XW + \ones b^T \]
For a linear layer, the partial derivatives are
\[ \frac{\partial z}{\partial X} = W \qquad \frac{\partial z}{\partial W} = X \qquad \frac{\partial z}{\partial b} = \ones \]
To compute derivatives of the loss function $L$, we apply the chain rule
\[ \frac{\partial L}{\partial X} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial X} \qquad \frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W} \qquad \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial b} \]
Of course, images are not row vectors, so they must be reshaped appropriately.

\subsection*{ReLU activation}
The forward pass of ReLU is $\max(0, x)$.\par
\bigskip
Its derivative is 1 for $x > 0$ and 0 for $x < 0$. At $x = 0$, its derivative does not exist, but its subderivative  lies between 0 and 1, so we simply set it to 0. We apply the chain rule just like the linear layers.

\section*{Q2}

\subsection* {Training Phase}
During training, the forward pass of inverted dropout will multiply the output of some neurons by 0, so it's effectively removing neurons. We can set the proportion $p$ of neurons to dropout, and scale the outputs of the remaining neurons by $1/(1-p)$ and act corresponding activations\par
During backward training, the 
\[ \frac{\partial L}{\partial X} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial X} =
\frac{\partial L}{\partial z} \frac{\partial 
\left \{
  \begin{tabular}{ccc}
  X$_i$$_j$ \qquad  \textit{if}  \textit{D}$_i$$_j$ = 1 \\
  0 \qquad \textit{if} \textit{D}$_i$$_j$ = 0 \\
  \end{tabular}
\right \}
}{\partial X} 
= 
\frac{\partial L}{\partial z} \frac{\partial 
\left \{
  \begin{tabular}{ccc}
  1 \qquad \textit{if}  \textit{D}$_i$$_j$ = 1 \\
  0 \qquad  \textit{if} \textit{D}$_i$$_j$ = 0 \\
  \end{tabular}
\right \}
}{\partial X}
= \frac{\partial L}{\partial z}\mathcal{D}
\]
\subsection* {Testing Phase}
Use all activations during forward pass of dropout.\par 
In backward dropout, \[ 
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial z} \ones= \frac{\partial L}{\partial z} \]

\begin{figure}
\centering
\includegraphics[width=\textwidth]{dropout.png}
\caption{Dropout Illustration}
\end{figure}

\section*{Q3}

The softmax function $\sigma : \Re^C \rightarrow [0, 1]^C$ can represent a probability distribution over $C$ classes:
\[ \sigma(z_1, \dots, z_C) = \frac{1}{\exp(z_1) + \dots + \exp(z_C)}
   \begin{bmatrix} \exp(z_1) \\ \vdots \\ \exp(z_C) \end{bmatrix} :=
   \begin{bmatrix} \sigma_1  \\ \vdots \\ \sigma_C  \end{bmatrix} \]
We used the ``normalization trick'' described in the coursework manual for numerical stability.\par
\bigskip
Derivatives of the softmax function are
\[ \frac{\partial \sigma_i}{\partial z_j} = (\delta_{i,j} - \sigma_j) \, \sigma_i \qquad \forall \, i, j \in \{1, \dots, C\} \]
where $\delta_{i,j} = 0$ if $i \neq j$, otherwise $\delta_{i,j} = 1$.\par
\bigskip
The cross entropy loss of $n$ images is
\[ L = -\frac1n \sum_{i=1}^n \left[ y_{i,1} \log \sigma_{i,1} + \dots + y_{i,C} \log \sigma_{i,C} \right] \]
where $y_{i,k} = 1$ if image $i$ belongs to class $k$, otherwise $y_{i,k} = 0$. Similarly, $\sigma_{i,k}$ is the softmax probability that image $i$ belongs to class $k$.\par
\bigskip
Derivatives of the loss function for $k \in \{1, \dots, C\}$ are
\begin{align*}
    \frac{\partial L}{\partial z_k}
    &= -\frac1n \sum_{i=1}^n \left[ \frac{y_{i,1}}{\sigma_{i,1}} \, \frac{\partial \sigma_{i,1}}{\partial z_k} + \dots + \frac{y_{i,C}}{\sigma_{i,C}} \, \frac{\partial \sigma_{i,C}}{\partial z_k} \right] \\
    &= -\frac1n \sum_{i=1}^n \left[ y_{i,1} \, (\delta_{1,k} - \sigma_{i,k}) + \dots + y_{i,C} \, (\delta_{C,k} - \sigma_{i,k}) \right] \\
    &= \frac1n \sum_{i=1}^n \left( \sigma_{i,k} - y_{i,1} \, \delta_{1,k} - \dots - y_{i,C} \, \delta_{C,k} \right) \qquad \qquad \text{since } y_{i,1} + \dots + y_{i,C} = 1
\end{align*}
Note that $y_{i,1} \, \delta_{1,k} + \dots + y_{i,C} \, \delta_{C,k}$ is 1 when $k$ is such that $y_{i,k} = 1$, and it is 0 for other values of $k$. So computing the derivative involves going through each row of
\[ \begin{bmatrix}
   \sigma_{1,1} & \dotsm & \sigma_{1,C} \\
   & \vdots & \\
   \sigma_{n,1} & \dotsm & \sigma_{n,C}
   \end{bmatrix} \]
and subtracting 1 from the appropriate column.

\section*{Q4}
The shapes of our input and output neurons are $32 \times 32 \times 3$ and 10 respectively.

\subsection*{Overfit}
To overfit 50 images in the CIFAR-10 dataset, we used the following architecture and hyperparameters:
\begin{description}
    \item[Activation Function:] ReLU
    \item[Hidden Neurons:] [50]
    \item[Regularization:] None
    \item[Update Rule:] Stochastic Gradient Descent
    \item[Epochs:] 20
    \item[Batch Size:] 100
    \item[Learning Rate:] 1e-3
    \item[Learning Rate Decay:] 0.95
\end{description}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{train1.png}
\caption{Overfit accuracy}
\end{figure}

\subsection*{Architecture}
To achieve at least 50\% accuracy on the CIFAR-10 dataset, we used the following architecture and hyperparameters:
\begin{description}
    \item[Activation Function:] ReLU
    \item[Hidden Neurons:] [50, 8]
    \item[Regularization:] L2 with factor of 0.3
    \item[Update Rule:] Stochastic Gradient Descent
    \item[Epochs:] 20
    \item[Batch Size:] 100
    \item[Learning Rate:] 5e-3
    \item[Learning Rate Decay:] 0.95
\end{description}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{train2.png}
\caption{accuracy on CIFAR10}
\end{figure}

\section*{Q5}

\subsection*{Step 1}
Network architecture: 1 hidden layer?\par
stopping criterion:\par
Learning rate update schedule:\par

\subsection*{Step 2}
We plotted validation loss against time for various learning rates: 0.05, 0.1, 0.2, etc. The best learning rate is one with lowest validation loss for a given training time i.e. it results in lowest asymptotic error and converges rapidly.\par
\bigskip
Include the plot for training and validation loss and report training and validation classification error.

\subsection*{Step 3}
Use dropout and report if there is any improvement in the validation performance

\subsection*{Step 4}
Use L2 and compare the performance with dropout.

\subsection*{Step 5}
Optimise the number of hidden layers and the number of neurons in each hidden layer.

\section*{A1}
In general, no algorithm will perform better than all other algorithms on all datasets (No free lunch theorem).\par
\bigskip
Moreover we need to tune hyperparameters, without doing so the algorithms have not been trained properly.

\section*{A2}
No change is needed since our functions are written to accommodate an arbitrary number of classes.

\end{document}
