\documentclass[12pt, a4paper]{article}
\usepackage[margin = 2cm]{geometry}
% Vertical text spacing
\parindent = 0cm \parskip = 0cm
% Section
\usepackage[compact]{titlesec} \titlespacing*{\section}{0pt}{2ex}{2ex}
\titleformat*{\section}{\normalfont\Large\bfseries\color[RGB]{0,0,192}}
\titleformat*{\subsection}{\normalfont\bfseries\color[RGB]{192,0,0}}
% Table spacing
\newcommand\TS{\rule{0pt}{2.6ex}}         % Top strut
\newcommand\BS{\rule[-0.9ex]{0pt}{0pt}}   % Bottom strut
\usepackage{array, multirow}
\newcolumntype{L}[1]{>{\raggedright\TS\BS\arraybackslash}m{#1}}
\newcolumntype{C}[1]{>{\centering  \TS\BS\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft \TS\BS\arraybackslash}m{#1}}
% Equations
\usepackage{amsmath, bm, bbold, tikz}

\title{\vspace{-6ex} CO395 Group 57 \vspace{-1ex}}
\author{Dongxiao Huang, Zheng Xun Phang, Yufeng Zhang}
\date{\vspace{-3ex}}

\begin{document}
\maketitle
\newcommand\ones{\bm{1}}

\section*{Q1}
\subsection*{Linear layers}
If each image in $X$ is reshaped into a row vector, then the forward pass can be written as
\[ z = XW + \ones b^T \]
For a linear layer, the partial derivatives are
\[ \frac{\partial z}{\partial X} = W \qquad \frac{\partial z}{\partial W} = X \qquad \frac{\partial z}{\partial b} = \ones \]
To compute derivatives of the loss function $L$, we apply the chain rule
\[ \frac{\partial L}{\partial X} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial X} \qquad \frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W} \qquad \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial b} \]
Of course, images are not row vectors, so they must be reshaped appropriately.

\subsection*{ReLU activation}
The forward pass of ReLU is $\max(0, x)$.\par
\bigskip
Its derivative is 1 for $x \geq 0$ and 0 for $x < 0$. We apply the chain rule just like the linear layers.

\section*{Q2}
During training, the forward pass of dropout will multiply the output of some neurons by 0, so it's effectively removing neurons. We can set the proportion $p$ of neurons to dropout, and scale the outputs of the remaining neurons by $1/(1-p)$.\par
\bigskip
During testing, we restore all neurons.

\section*{Q3}
The softmax function $\sigma : \Re^C \rightarrow [0, 1]^C$ can represent a probability distribution over $C$ classes:
\[ \sigma(z_1, \dots, z_C) = \frac{1}{\exp(z_1) + \dots + \exp(z_C)}
   \begin{bmatrix} \exp(z_1) \\ \vdots \\ \exp(z_C) \end{bmatrix} :=
   \begin{bmatrix} \sigma_1  \\ \vdots \\ \sigma_C  \end{bmatrix} \]
We used the ``normalization trick'' described in the coursework manual for numerical stability.\par
\bigskip
Derivatives of the softmax function are
\[ \frac{\partial \sigma_i}{\partial z_j} = (\delta_{ij} - \sigma_j) \, \sigma_i \qquad \forall \, i, j \in \{1, \dots, C\} \]
where $\delta_{i,j} = 0$ if $i \neq j$, otherwise $\delta_{i,j} = 1$.\par
\bigskip
The cross entropy loss of $n$ images is
\[ L = -\frac1n \sum_{i=1}^n \left[ y_{i,1} \log \sigma_{i,1} + \dots + y_{i,C} \log \sigma_{i,C} \right] \]
where $y_{i,k} = 1$ if image $i$ belongs to class $k$, otherwise $y_{i,k} = 0$. Similarly, $\sigma_{i,k}$ is the softmax probability that image $i$ belongs to class $k$.\par
\bigskip
Derivatives of the loss function for $k \in \{1, \dots, C\}$ are
\begin{align*}
    \frac{\partial L}{\partial z_k}
    &= -\frac1n \sum_{i=1}^n \left[ \frac{y_{i,1}}{\sigma_{i,1}} \, \frac{\partial \sigma_{i,1}}{\partial z_k} + \dots + \frac{y_{i,C}}{\sigma_{i,C}} \, \frac{\partial \sigma_{i,C}}{\partial z_k} \right]
    = -\frac1n \sum_{i=1}^n \left[ y_{i,1} \, (\delta_{1,k} - \sigma_k) + \dots + y_{i,C} \, (\delta_{C,k} - \sigma_k) \right] \\
    &= -\frac1n \sum_{i=1}^n \left( y_{i,1} \, \delta_{1,k} + \dots + y_{i,C} \, \delta_{C,k} - \sigma_k \right) \qquad \qquad \text{since } y_{i,1} + \dots + y_{i,C} = 1
\end{align*}

\section*{Q4}
CIFAR-10 data

\section*{Q5}

\section*{A1}

\section*{A2}

\end{document}
