\documentclass[12pt, a4paper]{article}
\usepackage[margin = 2cm]{geometry}
\usepackage{soul}
% Vertical text spacing
\parindent = 0cm \parskip = 0cm
% Section
\usepackage[compact]{titlesec} \titlespacing*{\section}{0pt}{2ex}{2ex}
\titleformat*{\section}{\normalfont\Large\bfseries\color[RGB]{0,0,192}}
\titleformat*{\subsection}{\normalfont\bfseries\color[RGB]{192,0,0}}
% Table spacing
\newcommand\TS{\rule{0pt}{2.6ex}}         % Top strut
\newcommand\BS{\rule[-0.9ex]{0pt}{0pt}}   % Bottom strut
\usepackage{array, multirow}
\newcolumntype{L}[1]{>{\raggedright\TS\BS\arraybackslash}m{#1}}
\newcolumntype{C}[1]{>{\centering  \TS\BS\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft \TS\BS\arraybackslash}m{#1}}
% Equations
\usepackage{amsmath, bm, bbold, tikz}

\title{\vspace{-6ex} CO395 Group 57 \vspace{-1ex}}
\author{Dongxiao Huang, Zheng Xun Phang, Yufeng Zhang}
\date{\vspace{-3ex}}

\begin{document}
\maketitle
\newcommand\ones{\bm{1}}

\section*{Q1}
\subsection*{Linear layers}
If each image in $X$ is reshaped into a row vector, then the forward pass can be written as
\[ z = XW + \ones b^T \]
For a linear layer, the partial derivatives are
\[ \frac{\partial z}{\partial X} = W \qquad \frac{\partial z}{\partial W} = X \qquad \frac{\partial z}{\partial b} = \ones \]
To compute derivatives of the loss function $L$, we apply the chain rule
\[ \frac{\partial L}{\partial X} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial X} \qquad \frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W} \qquad \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial b} \]
Of course, images are not row vectors, so they must be reshaped appropriately.

\subsection*{ReLU activation}
The forward pass of ReLU is $\max(0, x)$.\par
\bigskip
Its derivative is 1 for $x \geq 0$ and 0 for $x < 0$. We apply the chain rule just like the linear layers.

\section*{Q2}
During training, the forward pass of dropout will multiply the output of some neurons by 0, so it's effectively removing neurons. We can set the proportion $p$ of neurons to dropout, and scale the outputs of the remaining neurons by $1/(1-p)$.\par
\bigskip
During testing, we restore all neurons.

\section*{Q3}
The softmax function $\sigma : \Re^C \rightarrow [0, 1]^C$ can represent a probability distribution over $C$ classes:
\[ \sigma(z_1, \dotsm, z_C) = \frac{1}{\exp(z_1) + \dotsm + \exp(z_C)}
   \begin{bmatrix} \exp(z_1) \\ \vdots \\ \exp(z_C) \end{bmatrix} :=
   \begin{bmatrix} \sigma_1  \\ \vdots \\ \sigma_C  \end{bmatrix} \]
We applied the ``normalization trick'' described in the coursework manual for numerical stability. The derivatives of the softmax function are
\[ \frac{\partial \sigma_i}{\partial z_j} = (\delta_{ij} - \sigma_i) \, \sigma_j \qquad \forall i, j \in \{1, \dotsm, C\} \]
where $\delta_{ij} = 0$ if $i \neq j$ and $\delta_{ij} = 1$ otherwise. Let $y_{i,k} = 1$ if image $i$ belongs to class $k$, otherwise $y_{i,k} = 0$. The cross entropy loss is
\[ L = -\frac1n \sum_{i=0}^n \left[ y_{i,1} \log \sigma_{i,1} + \dotsm + y_{i,C} \log \sigma_{i,C} \right] \]

\section*{Q4}
CIFAR-10 data

\section*{Q5}

\section*{A1}

\section*{A2}

\end{document}
